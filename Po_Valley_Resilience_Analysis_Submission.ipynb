{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26496372",
   "metadata": {},
   "source": [
    "# Po Valley Resilience & Rice Monitoring Framework\n",
    "## Submission Overview\n",
    "\n",
    "**Objective:**\n",
    "We aim to collect data from disparate satellite sources, isolate specific crop windows, and extract distinctive crop characteristics to build a comprehensive crop monitoring framework.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "**Table 1: Study Regions**\n",
    "\n",
    "| Region | Location | Coordinates (BBox) | Purpose |\n",
    "|---|---|---|---|\n",
    "| **Region A** | Vercelli, Italy | `[8.34, 45.27, 8.40, 45.31]` | Primary focus area. Represents favorable, high-yield rice phenology. |\n",
    "| **Region B** | Pavia, Italy | `[9.022, 45.161, 9.055, 45.182]` | Secondary focus area. Represents environmentally stressed rice conditions for comparative analysis. |\n",
    "\n",
    "**Table 2: Sensor Characteristics & Objectives**\n",
    "\n",
    "| Sensor | Source Catalog | Target Data Layer | Derived Characteristic / Objective |\n",
    "|---|---|---|---|\n",
    "| **Sentinel-1 (Radar)** | EOPF STAC | VV / VH Backscatter | Determine physical canopy structure, flooded field status, and moisture penetration. |\n",
    "| **Sentinel-2 (Optical)** | Earth Search AWS | Red, NIR, SWIR Bands | Calculate NDVI (Vegetation Greenness), distinguishing crop health and growth cycles. |\n",
    "| **Sentinel-3 (Thermal)** | EOPF STAC | Land Surface Temp (LST) | Assess canopy heat stress and micro-climate anomalies over the season. |\n",
    "\n",
    "**Methodological Workflow:**\n",
    "1. **Setup & Initialization:** Load essential geospatial libraries and initiate parallel processing clusters.\n",
    "2. **Study Area Definition:** Establish precise geographic coordinates for the target regions.\n",
    "3. **Data Discovery & STAC API Setup:** Connect to both EOPF and Earth Search catalogs for multi-sensor comparisons.\n",
    "4. **Geospatial Visualization:** Validate geographic boundaries via interactive web mappings.\n",
    "5. **Data Extraction & Zarr Cube Generation:** Generate optimized, cloud-native analysis-ready data structures containing crop health metrics (NDVI).\n",
    "6. **Multi-Sensor Extension (S1 & S3):** Query radar and thermal data streams for structure and stress indicators.\n",
    "7. **Time-Series Metric Extraction:** Compute and map the temporal progression of crop indicators across the growing season.\n",
    "8. **Comprehensive Availability Coverage:** Review the statistical footprint and cross-sensor coverage available over the target sites.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02ae69",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 1: Setup & Initialization\n",
    "Import essential analytical tools and create a localized Dask cluster for parallel processing.\n",
    "\n",
    "**Expected Output:** Successful library instantiation, and initialization logs for the Dask cluster.\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies for the notebook directly into the active kernel environment\n",
    "!pip install -q pystac-client odc-stac folium xarray zarr s3fs dask fsspec seaborn geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Workaround for Anaconda PROJ version conflicts\n",
    "os.environ.pop('PROJ_LIB', None)\n",
    "os.environ.pop('PROJ_DATA', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "\n",
    "from pystac_client import Client\n",
    "import shutil\n",
    "\n",
    "import odc.stac\n",
    "\n",
    "import fsspec\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from pystac_client import CollectionSearch\n",
    "from shapely import geometry\n",
    "from distributed import LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=False)\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe72b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = CollectionSearch(\n",
    "    url=\"https://earth-search.aws.element84.com/v1/collections\",\n",
    ")\n",
    "for collection_dict in search.collections_as_dicts():\n",
    "    print(collection_dict[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170da077",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 2: Study Area Definition\n",
    "Here we define the precise coordinates (Bounding Boxes) and the date calendar for our analysis. \n",
    "\n",
    "- **Timeframe:** April 1st, 2025 to December 1st, 2025 (Capturing the entire rice planting, growing, and harvesting season).\n",
    "- **Cloud Filter:** We command the STAC catalog to only show us images that have less than 50% cloud cover.\n",
    "\n",
    "**Expected Output:** Assignment of geographic coordinates and calendar dates to Python variables.\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vercelli, Italy (Rice fields)\n",
    "REGION_A = [8.34, 45.27, 8.40, 45.31]\n",
    "DATE_RANGE_A = \"2025-04-01/2025-12-01\"\n",
    "\n",
    "# Pavia, Italy (Rice fields)\n",
    "REGION_B = [9.022, 45.161, 9.055, 45.182]\n",
    "DATE_RANGE_B = \"2025-04-01/2025-12-01\"\n",
    "\n",
    "CLOUD_COVER_LIMIT = 50  # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f159870",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 3: STAC Catalog Comparison\n",
    "STAC (SpatioTemporal Asset Catalog) is a standardized library for satellite imagery. Instead of downloading heavy files blindly, we use STAC APIs to \"ask\" the database what images are available over our study regions.\n",
    "\n",
    "We compare two major catalogs to select the best data source:\n",
    "1. **EOPF STAC:** Official European Space Agency (ESA) catalog.\n",
    "2. **Earth Search AWS:** Amazon's global catalog holding Cloud-Optimized GeoTIFFs (COGs).\n",
    "\n",
    "*The code block below will dynamically query both catalogs and generate a summary table comparing their coverage over Region A and Region B.*\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE BOTH STAC CATALOGS\n",
    "# ---------------------------------------------------------\n",
    "import pystac_client\n",
    "\n",
    "REGION = REGION_A\n",
    "DATES = DATE_RANGE_A\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: EOPF vs Earth Search AWS - REGION A (Vercelli)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. EOPF STAC\n",
    "print(\"\\n1. EOPF STAC (https://stac.core.eopf.eodc.eu)\")\n",
    "print(\"-\" * 60)\n",
    "eopf_client = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
    "eopf_search = eopf_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=REGION,\n",
    "    datetime=DATES,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": CLOUD_COVER_LIMIT}}\n",
    ")\n",
    "eopf_items = list(eopf_search.items())\n",
    "print(f\"\u2713 Total scenes found: {len(eopf_items)}\")\n",
    "print(f\"\u2713 Band naming: B02_10m, B03_10m, B04_10m, B08_10m, SCL_20m\")\n",
    "print(f\"\u2713 Location: European servers (EODC)\")\n",
    "print(f\"\u2713 Format: Original ESA Sentinel-2 naming\")\n",
    "\n",
    "# 2. Earth Search AWS\n",
    "print(\"\\n2. Earth Search AWS (https://earth-search.aws.element84.com/v1)\")\n",
    "print(\"-\" * 60)\n",
    "aws_client = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "aws_search = aws_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=REGION,\n",
    "    datetime=DATES,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": CLOUD_COVER_LIMIT}}\n",
    ")\n",
    "aws_items = list(aws_search.items())\n",
    "print(f\"\u2713 Total scenes found: {len(aws_items)}\")\n",
    "print(f\"\u2713 Band naming: red, nir, blue, green, scl (lowercase aliases)\")\n",
    "print(f\"\u2713 Location: AWS S3 (global)\")\n",
    "print(f\"\u2713 Format: Cloud-optimized COGs with simplified names\")\n",
    "\n",
    "# RECOMMENDATION\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\" * 60)\n",
    "if len(aws_items) > len(eopf_items):\n",
    "    print(f\"\ud83c\udfc6 Earth Search AWS has MORE scenes ({len(aws_items)} vs {len(eopf_items)}) on Region A\")\n",
    "    print(\"   \u2192 Better for comprehensive time series analysis\")\n",
    "    print(\"   \u2192 Faster global access via AWS\")\n",
    "    print(\"   \u2192 Simpler band naming (red, nir, scl)\")\n",
    "elif len(eopf_items) > len(aws_items):\n",
    "    print(f\"\ud83c\udfc6 EOPF has MORE scenes ({len(eopf_items)} vs {len(aws_items)})\")\n",
    "    print(\"   \u2192 Count: Better for European data\")\n",
    "    print(\"   \u2192 Official ESA naming convention\")\n",
    "else:\n",
    "    print(f\"Both have the same number of scenes ({len(aws_items)})\")\n",
    "    print(\"   \u2192 Choose based on access speed and naming preference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE BOTH STAC CATALOGS\n",
    "# ---------------------------------------------------------\n",
    "import pystac_client\n",
    "\n",
    "REGION = REGION_B\n",
    "DATES = DATE_RANGE_B\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: EOPF vs Earth Search AWS - REGION B (Pavia)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. EOPF STAC\n",
    "print(\"\\n1. EOPF STAC (https://stac.core.eopf.eodc.eu)\")\n",
    "print(\"-\" * 60)\n",
    "eopf_client = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
    "eopf_search = eopf_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=REGION,\n",
    "    datetime=DATES,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": CLOUD_COVER_LIMIT}}\n",
    ")\n",
    "eopf_items_b = list(eopf_search.items())\n",
    "print(f\"\u2713 Total scenes found: {len(eopf_items_b)}\")\n",
    "print(f\"\u2713 Band naming: B02_10m, B03_10m, B04_10m, B08_10m, SCL_20m\")\n",
    "print(f\"\u2713 Location: European servers (EODC)\")\n",
    "print(f\"\u2713 Format: Original ESA Sentinel-2 naming\")\n",
    "\n",
    "# 2. Earth Search AWS\n",
    "print(\"\\n2. Earth Search AWS (https://earth-search.aws.element84.com/v1)\")\n",
    "print(\"-\" * 60)\n",
    "aws_client = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "aws_search = aws_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=REGION,\n",
    "    datetime=DATES,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": CLOUD_COVER_LIMIT}}\n",
    ")\n",
    "aws_items_b = list(aws_search.items())\n",
    "print(f\"\u2713 Total scenes found: {len(aws_items_b)}\")\n",
    "print(f\"\u2713 Band naming: red, nir, blue, green, scl (lowercase aliases)\")\n",
    "print(f\"\u2713 Location: AWS S3 (global)\")\n",
    "print(f\"\u2713 Format: Cloud-optimized COGs with simplified names\")\n",
    "\n",
    "# RECOMMENDATION\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\" * 60)\n",
    "if len(aws_items_b) > len(eopf_items_b):\n",
    "    print(f\"\ud83c\udfc6 Earth Search AWS has MORE scenes ({len(aws_items_b)} vs {len(eopf_items_b)}) on Region B\")\n",
    "    print(\"   \u2192 Better for comprehensive time series analysis\")\n",
    "    print(\"   \u2192 Faster global access via AWS\")\n",
    "    print(\"   \u2192 Simpler band naming (red, nir, scl)\")\n",
    "elif len(eopf_items_b) > len(aws_items_b):\n",
    "    print(f\"\ud83c\udfc6 EOPF has MORE scenes ({len(eopf_items_b)} vs {len(aws_items_b)})\")\n",
    "    print(\"   \u2192 Better for European data\")\n",
    "    print(\"   \u2192 Official ESA naming convention\")\n",
    "else:\n",
    "    print(f\"Both have the same number of scenes ({len(aws_items_b)})\")\n",
    "    print(\"   \u2192 Choose based on access speed and naming preference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dynamic comparison table based on the items found above\n",
    "summary_data = {\n",
    "    \"Feature\": [\n",
    "        \"Scenes Found (Region A)\", \n",
    "        \"Scenes Found (Region B)\", \n",
    "        \"Band Naming\", \n",
    "        \"Storage Location\", \n",
    "        \"Format\"\n",
    "    ],\n",
    "    \"EOPF STAC\": [\n",
    "        len(eopf_items),  # Dynamically pull from Region A query\n",
    "        len(eopf_items_b),\n",
    "        \"Official ESA (e.g., B02_10m)\", \n",
    "        \"European Servers (EODC)\", \n",
    "        \"Standard\"\n",
    "    ],\n",
    "    \"Earth Search AWS\": [\n",
    "        len(aws_items),   # Dynamically pull from Region A query\n",
    "        len(aws_items_b),\n",
    "        \"Simplified Aliases (e.g., blue)\", \n",
    "        \"Global AWS S3\", \n",
    "        \"Cloud-Optimized (Faster loading)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"STAC CATALOG COMPARISON SUMMARY\")\n",
    "print(\"============================================================\")\n",
    "display(df_comparison)\n",
    "\n",
    "print(\"\\nConclusion: Earth Search AWS is selected due to significantly higher valid scene counts and cloud-optimized formatting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9b681",
   "metadata": {},
   "source": [
    "### Monthly Coverage Check\n",
    "Before proceeding, we tally the exact number of valid Sentinel-2 scenes available per month across the 2025 growing season (April -> November) from our chosen catalog (Earth Search AWS). \n",
    "\n",
    "This confirms we have consistent satellite coverage throughout the entire crop cycle for both **Region A** and **Region B**, ensuring our final time-series charts will not have massive gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deed4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import pandas as pd\n",
    "\n",
    "# STAC API Setup\n",
    "STAC_URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "\n",
    "# Corrected Collection Variables\n",
    "COLLECTIONS = {\n",
    "    \"Sentinel-2 (L2A)\": \"sentinel-2-l2a\"\n",
    "}\n",
    "\n",
    "# Connect to Client once\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "\n",
    "for friendly_name, collection_id in COLLECTIONS.items():\n",
    "    print(f\"\\n--- Searching {friendly_name} from Earth Search AWS [Region A] ---\")\n",
    "    \n",
    "    search = catalog.search(\n",
    "        collections=[collection_id],\n",
    "        bbox=REGION_A,\n",
    "        datetime=DATE_RANGE_A,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": CLOUD_COVER_LIMIT}}\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    print(f\"Total items found: {len(items)}\")\n",
    "    \n",
    "    if items:\n",
    "        # Extract datetimes from items\n",
    "        # STAC items always have a .datetime property (UTC)\n",
    "        dates = [item.datetime for item in items]\n",
    "        \n",
    "        # Create a Pandas DataFrame for easy grouping\n",
    "        df = pd.DataFrame(dates, columns=['date'])\n",
    "        \n",
    "        # Convert to datetime objects if not already\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Group by Year-Month and count\n",
    "        # 'M' stands for Month end frequency\n",
    "        monthly_counts_a = df.groupby(df['date'].dt.strftime('%m-%Y')).size()\n",
    "        \n",
    "        print(\"Items per month:\")\n",
    "        print(monthly_counts_a)\n",
    "    else:\n",
    "        print(\"No items found in this range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Client once\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "\n",
    "for friendly_name, collection_id in COLLECTIONS.items():\n",
    "    print(f\"\\n--- Searching {friendly_name} from Earth Search AWS [Region B] ---\")\n",
    "    \n",
    "    search = catalog.search(\n",
    "        collections=[collection_id],\n",
    "        bbox=REGION_B,\n",
    "        datetime=DATE_RANGE_B,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": CLOUD_COVER_LIMIT}}\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    print(f\"Total items found: {len(items)}\")\n",
    "    \n",
    "    if items:\n",
    "        # Extract datetimes from items\n",
    "        # STAC items always have a .datetime property (UTC)\n",
    "        dates = [item.datetime for item in items]\n",
    "        \n",
    "        # Create a Pandas DataFrame for easy grouping\n",
    "        df = pd.DataFrame(dates, columns=['date'])\n",
    "        \n",
    "        # Convert to datetime objects if not already\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Group by Year-Month and count\n",
    "        # 'M' stands for Month end frequency\n",
    "        monthly_counts_b = df.groupby(df['date'].dt.strftime('%m-%Y')).size()\n",
    "        \n",
    "        print(\"Items per month:\")\n",
    "        print(monthly_counts_b)\n",
    "    else:\n",
    "        print(\"No items found in this range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stac_dataframe_summary2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_summary = pd.DataFrame({\n",
    "    'Region A': monthly_counts_a,\n",
    "    'Region B': monthly_counts_b\n",
    "})\n",
    "\n",
    "df_summary.index.name = 'MMYY'\n",
    "df_summary = df_summary.fillna(0).astype(int)\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"Summary of Sentinel-2 (L2A) from Earth Search AWS in [Region A] and [Region B]\")\n",
    "print(\"============================================================\")\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca8be6",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 4: Geospatial Visualization\n",
    "Before we start crunching massive amounts of data, it's crucial to visually verify that our Bounding Boxes (`REGION_A` and `REGION_B`) actually cover the agricultural fields we want to analyze. \n",
    "\n",
    "We use **Folium**, an interactive mapping library, to overlay our coordinates onto a real-world map. \n",
    "*Note: You can zoom in and drag the map around.*\n",
    "\n",
    "**Expected Output:** Interactive Folium widgets showing red rectangles over the target Italian farming regions.\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = REGION_A\n",
    "center = [(min_lat + max_lat)/2, (min_lon + max_lon)/2]\n",
    "\n",
    "m = folium.Map(location=center, zoom_start=14, tiles='CartoDB positron')\n",
    "folium.Rectangle(\n",
    "    bounds=[(min_lat, min_lon), (max_lat, max_lon)],\n",
    "    color=\"red\", fill=True, fill_opacity=0.15\n",
    ").add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040cd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = REGION_B\n",
    "center = [(min_lat + max_lat)/2, (min_lon + max_lon)/2]\n",
    "\n",
    "m = folium.Map(location=center, zoom_start=14, tiles='CartoDB positron')\n",
    "folium.Rectangle(\n",
    "    bounds=[(min_lat, min_lon), (max_lat, max_lon)],\n",
    "    color=\"red\", fill=True, fill_opacity=0.15\n",
    ").add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110ada7",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 5: Data Extraction & Zarr Cube Generation (From Earch Search AWS)\n",
    "\n",
    "This is the most critical step in the notebook. Now that we know Earth Search AWS has the best optical coverage, we will:\n",
    "\n",
    "1. **Lazy Load (odc.stac):** We tell the computer to \"peek\" at the satellite images (Red, Near-Infrared, and Scene Classification mask) without downloading the heavy files directly into your computer's RAM. \n",
    "2. **Masking (Cloud Removal):** Satellite images often have clouds or shadows blocking the farms. We use the Scene Classification band (`scl`) to strip away clouds, keeping only clear vegetation, bare soil, and water.\n",
    "3. **Calculate NDVI (Crop Health):** NDVI (Normalized Difference Vegetation Index) is the standard formula for plant health. It mathematically compares Near-Infrared light (which healthy leaves reflect) with Red light (which plants absorb). *Higher values = greener, healthier crops.*\n",
    "4. **Save to Zarr:** We take this massive, multi-dimensional block of calculated NDVI data (X, Y, and Time) and save it to your hard drive as a `.zarr` folder. Zarr is a highly compressed, cloud-optimized format that makes plotting the data lightning fast later.\n",
    "\n",
    "**Expected Output:** The code will process hundreds of Gigabytes of remote data, reduce it to just NDVI, and save a persistent Zarr directory for both Region A and Region B.\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33be189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECT AVAILABLE BANDS\n",
    "# ---------------------------------------------------------\n",
    "print(\"Checking available bands/assets in the first item...\")\n",
    "if len(items) > 0:\n",
    "    first_item = items[0]\n",
    "    print(\"\\nAvailable assets:\")\n",
    "    for asset_key in first_item.assets.keys():\n",
    "        print(f\"  - {asset_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716ab9f",
   "metadata": {},
   "source": [
    "### Step 6: NVDI: Visualizing the Growth Cycle\n",
    "\n",
    "Based on the processed Zarr data, we can now map the true vegetative growth cycle of the region. Our methodology strictly separates verified sensor data from mathematical estimation:\n",
    "\n",
    "1. **Primary Data Source:** We use the local `.zarr` files generated in the previous step, built exclusively from the high-frequency Earth Search AWS catalog.\n",
    "2. **Cloud Processing (Masking):** The Zarr engine already parsed the `scl` (Scene Classification Layer) band. All pixels identified as cloud or cloud shadow have been intentionally dropped (marked as `NaN`) to prevent contaminated pixels from ruining the vegetative health score.\n",
    "3. **Filtering and Interpolation:** \n",
    "    - **Ground Truth (Green Dots):** The remaining, verified clear-sky averages are plotted as distinct points. If a satellite pass was completely obscured by clouds over the Region bounding box, no dot is drawn.\n",
    "    - **Continuous Estimation (Orange Line):** We perform a linear interpolation across the missing (cloudy) dates to provide a visual approximation of the continuous crop phenology.\n",
    "    - **Data Integrity:** The final tabular output permanently drops completely obscured records, presenting only the valid ground truth alongside our interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ds_local = xr.open_dataset(zarr_path, engine='zarr', decode_cf=True)\n",
    "\n",
    "# Calculate Spatial mean for time series (this contains NaNs where clouds were masked)\n",
    "ts = ds_local.NDVI.mean(dim=[\"x\", \"y\"], skipna=True)\n",
    "\n",
    "# 1. Convert to a Pandas Series for easy interpolation\n",
    "df_ts = ts.to_pandas()\n",
    "\n",
    "# 2. Interpolate the missing NaN values (linear interpolation connects the dots)\n",
    "df_ts_interpolated = df_ts.interpolate(method='linear')\n",
    "\n",
    "# 3. Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the interpolated data as a continuous orange line\n",
    "df_ts_interpolated.plot(kind='line', color='orange', linestyle='--', label='Interpolated Data')\n",
    "\n",
    "# Plot the original data as green dots (no line, just the valid points)\n",
    "df_ts.plot(style='o', color='green', label='Original Data (Cloud-free)')\n",
    "\n",
    "plt.title(\"Phenology (growth cycle) of the rice fields (April to December)\")\n",
    "plt.ylabel(\"NDVI\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Display a table of the results\n",
    "print(\"\\n============================================================\")\n",
    "print(f\"NDVI Time Series Values ({zarr_path})\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "# Create a clean DataFrame for display\n",
    "df_results = pd.DataFrame({\n",
    "    'Original NDVI': df_ts.round(3),\n",
    "    'Interpolated NDVI': df_ts_interpolated.round(3)\n",
    "})\n",
    "\n",
    "# Filter out rows where both are NaN just to keep table clean (optional, but good practice)\n",
    "df_results = df_results.dropna(subset=['Interpolated NDVI'])\n",
    "\n",
    "# Show as an interactive table\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Workaround for Anaconda PROJ version conflicts\n",
    "os.environ.pop('PROJ_LIB', None)\n",
    "os.environ.pop('PROJ_DATA', None)\n",
    "import os\n",
    "import shutil\n",
    "from pystac_client import Client\n",
    "import odc.stac\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "STAC_URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# pavia, Italy (Rice fields)\n",
    "BBOX = [9.022, 45.161, 9.055, 45.182]\n",
    "DATES = \"2025-04-01/2025-12-01\"\n",
    "\n",
    "print(\"Searching STAC catalog...\")\n",
    "client = Client.open(STAC_URL)\n",
    "\n",
    "search = client.search(\n",
    "    collections=[COLLECTION],\n",
    "    bbox=BBOX,\n",
    "    datetime=DATES,\n",
    "    # Relaxed cloud cover slightly to ensure we get hits, \n",
    "    # we handle masking later anyway.\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 50}} \n",
    ")\n",
    "\n",
    "items = list(search.item_collection())\n",
    "print(f\"Found {len(items)} scenes.\")\n",
    "\n",
    "# --- SAFETY CHECK ---\n",
    "if len(items) == 0:\n",
    "    raise ValueError(\"No scenes found! Try increasing the date range or removing the cloud filter.\")\n",
    "\n",
    "# 2. LAZY LOAD DATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"Configuring lazy load...\")\n",
    "ds = odc.stac.load(\n",
    "    items,\n",
    "    bands=[\"red\", \"nir\", \"scl\"], # Note: Earth Search uses lowercase aliases often, but B04/B08 work too\n",
    "    bbox=BBOX,\n",
    "    resolution=10,\n",
    "    chunks={\"x\": 1024, \"y\": 1024, \"time\": 1},\n",
    "    groupby=\"solar_day\",\n",
    "    crs=\"EPSG:32632\"\n",
    ")\n",
    "\n",
    "# 3. MASKING & NDVI\n",
    "# ---------------------------------------------------------\n",
    "# Earth Search SCL: 0=No Data, 1=Saturated, 3=Shadow, 8-10=Clouds, 4=Veg, 5=Bare, 6=Water\n",
    "valid_mask = (ds.scl == 4) | (ds.scl == 5) | (ds.scl == 6) | (ds.scl == 7)\n",
    "\n",
    "# Scale factors: Earth Search data is usually uint16 (0-10000)\n",
    "red = ds.red.where(valid_mask) / 10000.0\n",
    "nir = ds.nir.where(valid_mask) / 10000.0\n",
    "\n",
    "ndvi = (nir - red) / (nir + red)\n",
    "ndvi = ndvi.rename(\"NDVI\")\n",
    "\n",
    "# 4. SAVE TO ZARR\n",
    "# ---------------------------------------------------------\n",
    "zarr_path = \"rice_ndvi_cube_region_b.zarr\"\n",
    "if os.path.exists(zarr_path):\n",
    "    shutil.rmtree(zarr_path)\n",
    "\n",
    "print(f\"Computing and saving to {zarr_path}...\")\n",
    "# This is the heavy lifting step\n",
    "d = ndvi.to_dataset()\n",
    "# Forcefully remove all metadata to bypass xarray/zarr Float32 bug\n",
    "d.attrs.clear()\n",
    "for var in d.variables:\n",
    "    d[var].attrs.clear()\n",
    "    d[var].encoding = {'_FillValue': None}\n",
    "# Save using safely configured engine\n",
    "d.to_zarr(zarr_path, mode=\"w\", safe_chunks=False)\n",
    "print(\"Zarr write complete.\")\n",
    "\n",
    "# 5. PLOT (Using the saved Zarr)\n",
    "# ---------------------------------------------------------\n",
    "ds_local = xr.open_dataset(zarr_path, engine='zarr', decode_cf=False, mask_and_scale=False)\n",
    "\n",
    "# Spatial mean for time series\n",
    "ts = ds_local.NDVI.mean(dim=[\"x\", \"y\"], skipna=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ts.plot.line(marker=\"o\", color=\"green\")\n",
    "plt.title(\"NDVI Time Series (Rice Detection)\")\n",
    "plt.ylabel(\"NDVI\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f38860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: Continuous lines use linear interpolation to bridge gaps caused by cloud-cover days.\n",
    "# Load both regions' NDVI data from Zarr\n",
    "import pandas as pd\n",
    "\n",
    "# Region A data\n",
    "ds_region_a = xr.open_dataset(\"rice_ndvi_cube_region_a.zarr\", engine='zarr', decode_cf=False, mask_and_scale=False)\n",
    "ts_a = ds_region_a.NDVI.mean(dim=[\"x\", \"y\"], skipna=True)\n",
    "data_region_a = pd.DataFrame({\n",
    "    'date': ts_a.time.values,\n",
    "    'ndvi_mean': ts_a.values\n",
    "})\n",
    "\n",
    "# Region B data\n",
    "ds_region_b = xr.open_dataset(\"rice_ndvi_cube_region_b.zarr\", engine='zarr', decode_cf=False, mask_and_scale=False)\n",
    "ts_b = ds_region_b.NDVI.mean(dim=[\"x\", \"y\"], skipna=True)\n",
    "data_region_b = pd.DataFrame({\n",
    "    'date': ts_b.time.values,\n",
    "    'ndvi_mean': ts_b.values\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot Region A\n",
    "ax.plot(data_region_a['date'], data_region_a['ndvi_mean'].interpolate(method='linear'), \n",
    "        marker='o', linewidth=2.5, markersize=8, \n",
    "        label='Region A (Favorable)', color='#2ecc71')\n",
    "\n",
    "# Plot Region B\n",
    "ax.plot(data_region_b['date'], data_region_b['ndvi_mean'].interpolate(method='linear'), \n",
    "        marker='s', linewidth=2.5, markersize=8, \n",
    "        label='Region B (Stress)', color='#e74c3c')\n",
    "\n",
    "# Add reference line for critical NDVI threshold\n",
    "ax.axhline(y=0.6, color='gray', linestyle='--', linewidth=1.5, alpha=0.6, \n",
    "           label='Optimal NDVI threshold')\n",
    "\n",
    "ax.set_xlabel('Month (2025)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('NDVI (Normalized Difference Vegetation Index)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Crop Vegetation Dynamics: Region A vs Region B', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.3, 0.85)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Statistics:\")\n",
    "print(f\"  Region A - Peak NDVI: {data_region_a['ndvi_mean'].interpolate(method='linear').max():.3f}\")\n",
    "print(f\"  Region B - Peak NDVI: {data_region_b['ndvi_mean'].interpolate(method='linear').max():.3f}\")\n",
    "print(f\"  NDVI Difference: {(data_region_a['ndvi_mean'].interpolate(method='linear').max() - data_region_b['ndvi_mean'].interpolate(method='linear').max()):.3f} ({((data_region_a['ndvi_mean'].interpolate(method='linear').max() - data_region_b['ndvi_mean'].interpolate(method='linear').max()) / data_region_b['ndvi_mean'].interpolate(method='linear').max() * 100):.1f}% higher in Region A)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787ad53",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 7: VV Backscatter: Multi-Sensor Extension (Sentinel-1 Radar)\n",
    "\n",
    "Optical sensors (like Sentinel-2 above) are great for seeing \"greenness\", but they are completely blind when it's cloudy. \n",
    "\n",
    "Here we bring in **Sentinel-1**, which uses **Radar (SAR)**. Radar shoots microwaves down to Earth and measures how they bounce back (backscatter). Microwaves pierce right through clouds and rain! \n",
    "\n",
    "**Why use Radar for Rice?**\n",
    "Rice is grown in flooded fields. \n",
    "- When fields are flooded (water), the radar bounces away like a mirror (Low VV Backscatter).\n",
    "- When the rice grows tall out of the water, it scatters the radar effectively (High VV Backscatter).\n",
    "By looking at the radar timeline, we can actually \"see\" the exact week the farmers flooded their fields and when the crop canopy closed over the water!\n",
    "\n",
    "**Expected Output:** Statistical processing of radar backscatter over our study areas.\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fe713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "STAC_URL = \"https://stac.core.eopf.eodc.eu\"\n",
    "COLLECTION_ID = \"sentinel-1-l1-grd\"\n",
    "DATE_RANGE = \"2020-04-01/2025-12-01\"\n",
    "\n",
    "REGIONS = {\n",
    "    \"REGION_A-italy\": [8.34, 45.27, 8.40, 45.31],\n",
    "    \"REGION_B-spain\": [0.55, 40.58, 0.90, 40.83],\n",
    "}\n",
    "\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "\n",
    "def monthly_counts_for_bbox(bbox, date_range):\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION_ID],\n",
    "        bbox=bbox,\n",
    "        datetime=date_range\n",
    "    )\n",
    "    items = list(search.items())\n",
    "    if not items:\n",
    "        return items, pd.Series(dtype=int)\n",
    "\n",
    "    dates = [it.datetime for it in items]\n",
    "    df = pd.DataFrame({\"date\": pd.to_datetime(dates, utc=True)})\n",
    "    # group by month (PeriodIndex), then convert to Timestamp later for plotting/merging\n",
    "    monthly = df.groupby(df[\"date\"].dt.to_period(\"M\")).size().sort_index()\n",
    "    return items, monthly\n",
    "\n",
    "all_monthlies = {}\n",
    "all_items = {}\n",
    "\n",
    "print(\"--- Searching Sentinel-1 (GRD) on EOPF STAC ---\")\n",
    "for name, bbox in REGIONS.items():\n",
    "    print(f\"\\n[{name}] bbox={bbox}  datetime={DATE_RANGE}\")\n",
    "    items, monthly = monthly_counts_for_bbox(bbox, DATE_RANGE)\n",
    "\n",
    "    all_items[name] = items\n",
    "    all_monthlies[name] = monthly\n",
    "\n",
    "    print(f\"Total items found: {len(items)}\")\n",
    "    if len(items) == 0:\n",
    "        print(\"No items found for this region.\")\n",
    "    else:\n",
    "        print(\"Items per month (head):\")\n",
    "        print(monthly.head(12))\n",
    "\n",
    "# ---- combine into one table aligned by month ----\n",
    "# Convert each Series' PeriodIndex to TimestampIndex so we can align across regions\n",
    "aligned = []\n",
    "for name, s in all_monthlies.items():\n",
    "    if len(s) == 0:\n",
    "        aligned.append(pd.Series(name=name, dtype=int))\n",
    "    else:\n",
    "        s2 = s.copy()\n",
    "        s2.index = s2.index.to_timestamp()  # Period -> Timestamp\n",
    "        s2.name = name\n",
    "        aligned.append(s2)\n",
    "\n",
    "counts_df = pd.concat(aligned, axis=1).fillna(0).astype(int)\n",
    "counts_df = counts_df.sort_index()\n",
    "\n",
    "print(\"\\n--- Combined monthly counts (tail) ---\")\n",
    "print(counts_df.tail(12))\n",
    "\n",
    "# ---- one bar chart for both regions ----\n",
    "plt.figure(figsize=(14, 6))\n",
    "counts_df.plot(kind=\"bar\", width=0.9)  # grouped bars per month for both regions\n",
    "plt.title(\"Sentinel-1 GRD availability per month (Italy vs Spain)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of scenes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- inspect assets (one sample per region) ----\n",
    "for name, items in all_items.items():\n",
    "    if not items:\n",
    "        continue\n",
    "\n",
    "    sample = items[0]\n",
    "    print(f\"\\n===== Sample inspection: {name} =====\")\n",
    "    print(\"Sample item id:\", sample.id)\n",
    "    print(\"Sample item datetime (UTC):\", sample.datetime)\n",
    "\n",
    "    print(\"\\nAsset keys in sample item:\")\n",
    "    for k, a in sample.assets.items():\n",
    "        mt = a.media_type if a.media_type else \"None\"\n",
    "        href = a.href if a.href else \"\"\n",
    "        print(f\" - {k:20s} | type={mt} | href={href[:120]}...\")\n",
    "\n",
    "    zarr_like = [\n",
    "        (k, a.href) for k, a in sample.assets.items()\n",
    "        if ((a.media_type and \"zarr\" in a.media_type.lower()) or (a.href and a.href.lower().endswith(\".zarr\")))\n",
    "    ]\n",
    "    print(\"\\nZarr-like assets detected:\" if zarr_like else \"\\nNo obvious zarr-like assets detected in sample.\")\n",
    "    for k, href in zarr_like:\n",
    "        print(f\" - {k}: {href}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "STAC_URL = \"https://stac.core.eopf.eodc.eu\"\n",
    "COLLECTION_ID = \"sentinel-3-slstr-l2-lst\"\n",
    "DATE_RANGE = \"2020-04-01/2025-12-01\"\n",
    "\n",
    "REGIONS = {\n",
    "    \"REGION_A-italy\": [8.34, 45.27, 8.40, 45.31],\n",
    "    \"REGION_B-spain\": [0.55, 40.58, 0.90, 40.83],\n",
    "}\n",
    "\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "\n",
    "def monthly_counts_for_bbox(bbox, date_range):\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION_ID],\n",
    "        bbox=bbox,\n",
    "        datetime=date_range\n",
    "    )\n",
    "    items = list(search.items())\n",
    "    if not items:\n",
    "        return items, pd.Series(dtype=int)\n",
    "\n",
    "    dates = [it.datetime for it in items]\n",
    "    df = pd.DataFrame({\"date\": pd.to_datetime(dates, utc=True)})\n",
    "    # group by month (PeriodIndex), then convert to Timestamp later for plotting/merging\n",
    "    monthly = df.groupby(df[\"date\"].dt.to_period(\"M\")).size().sort_index()\n",
    "    return items, monthly\n",
    "\n",
    "all_monthlies = {}\n",
    "all_items = {}\n",
    "\n",
    "print(\"--- Searching Sentinel-3 (GRD) on EOPF STAC ---\")\n",
    "for name, bbox in REGIONS.items():\n",
    "    print(f\"\\n[{name}] bbox={bbox}  datetime={DATE_RANGE}\")\n",
    "    items, monthly = monthly_counts_for_bbox(bbox, DATE_RANGE)\n",
    "\n",
    "    all_items[name] = items\n",
    "    all_monthlies[name] = monthly\n",
    "\n",
    "    print(f\"Total items found: {len(items)}\")\n",
    "    if len(items) == 0:\n",
    "        print(\"No items found for this region.\")\n",
    "    else:\n",
    "        print(\"Items per month (head):\")\n",
    "        print(monthly.head(12))\n",
    "\n",
    "# ---- combine into one table aligned by month ----\n",
    "# Convert each Series' PeriodIndex to TimestampIndex so we can align across regions\n",
    "aligned = []\n",
    "for name, s in all_monthlies.items():\n",
    "    if len(s) == 0:\n",
    "        aligned.append(pd.Series(name=name, dtype=int))\n",
    "    else:\n",
    "        s2 = s.copy()\n",
    "        s2.index = s2.index.to_timestamp()  # Period -> Timestamp\n",
    "        s2.name = name\n",
    "        aligned.append(s2)\n",
    "\n",
    "counts_df = pd.concat(aligned, axis=1).fillna(0).astype(int)\n",
    "counts_df = counts_df.sort_index()\n",
    "\n",
    "print(\"\\n--- Combined monthly counts (tail) ---\")\n",
    "print(counts_df.tail(12))\n",
    "\n",
    "# ---- one bar chart for both regions ----\n",
    "plt.figure(figsize=(14, 6))\n",
    "counts_df.plot(kind=\"bar\", width=0.9)  # grouped bars per month for both regions\n",
    "plt.title(\"Sentinel-3 LST availability per month (Italy vs Spain)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of scenes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- inspect assets (one sample per region) ----\n",
    "for name, items in all_items.items():\n",
    "    if not items:\n",
    "        continue\n",
    "\n",
    "    sample = items[0]\n",
    "    print(f\"\\n===== Sample inspection: {name} =====\")\n",
    "    print(\"Sample item id:\", sample.id)\n",
    "    print(\"Sample item datetime (UTC):\", sample.datetime)\n",
    "\n",
    "    print(\"\\nAsset keys in sample item:\")\n",
    "    for k, a in sample.assets.items():\n",
    "        mt = a.media_type if a.media_type else \"None\"\n",
    "        href = a.href if a.href else \"\"\n",
    "        print(f\" - {k:20s} | type={mt} | href={href[:120]}...\")\n",
    "\n",
    "    zarr_like = [\n",
    "        (k, a.href) for k, a in sample.assets.items()\n",
    "        if ((a.media_type and \"zarr\" in a.media_type.lower()) or (a.href and a.href.lower().endswith(\".zarr\")))\n",
    "    ]\n",
    "    print(\"\\nZarr-like assets detected:\" if zarr_like else \"\\nNo obvious zarr-like assets detected in sample.\")\n",
    "    for k, href in zarr_like:\n",
    "        print(f\" - {k}: {href}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129553e",
   "metadata": {},
   "source": [
    "### =============================================================\n",
    "### Step 7: Time-Series Metric Extraction (The Results)\n",
    "\n",
    "Now we load the `Zarr` data cubes we saved in Step 5 and plot them on a timeline. By putting Region A (Favorable) and Region B (Stressed) on the same graph, we can visually compare their growing seasons. \n",
    "\n",
    "*Note: You might see gaps or straight lines connecting dots. Because we removed clouds in Step 5, some weeks have no optical data. The code automatically connects the dots (interpolates) to show the general trend.*\n",
    "\n",
    "**Expected Output:** Time-series line charts showing the phenological (growth) cycle of the rice crops across the year.\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pystac_client import Client\n",
    "\n",
    "# Use the regions from earlier\n",
    "REGION_A_BBOX = [8.34, 45.27, 8.40, 45.31]  # Vercelli, Italy\n",
    "REGION_B_BBOX = [9.022, 45.161, 9.055, 45.182]  # Pavia, Italy\n",
    "\n",
    "# Date range for Sentinel-1 data\n",
    "S1_DATE_RANGE = \"2024-04-01/2025-12-01\"\n",
    "\n",
    "STAC_URL = \"https://stac.core.eopf.eodc.eu\"\n",
    "COLLECTION_ID = \"sentinel-1-l1-grd\"\n",
    "\n",
    "print(\"Fetching Sentinel-1 GRD data for Region A and Region B...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_vv_mean_odc(bbox, date_range, region_name):\n",
    "    \"\"\"\n",
    "    Fetch Sentinel-1 GRD data using odc.stac and extract mean VV backscatter values\n",
    "    \"\"\"\n",
    "    catalog = Client.open(STAC_URL)\n",
    "    \n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION_ID],\n",
    "        bbox=bbox,\n",
    "        datetime=date_range\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    print(f\"\\n[{region_name}]\")\n",
    "    print(f\"  Found {len(items)} Sentinel-1 scenes\")\n",
    "    \n",
    "    if len(items) == 0:\n",
    "        print(f\"  No data found for {region_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Check assets in first item\n",
    "    sample = items[0]\n",
    "    print(f\"  Available assets: {list(sample.assets.keys())}\")\n",
    "    \n",
    "    try:\n",
    "        # Try using odc.stac to load the data\n",
    "        print(f\"  Attempting to load VV band using odc.stac...\")\n",
    "        ds = odc.stac.load(\n",
    "            items,\n",
    "            bands=[\"vv\"],\n",
    "            bbox=bbox,\n",
    "            resolution=10,  # 10m resolution for Sentinel-1\n",
    "            chunks={\"x\": 512, \"y\": 512, \"time\": 1},\n",
    "            groupby=\"solar_day\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  Successfully loaded dataset\")\n",
    "        print(f\"  Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"  Dataset shape: {ds.dims}\")\n",
    "        \n",
    "        # Extract VV data and compute spatial mean for each time step\n",
    "        vv_data = ds.vv.mean(dim=[\"x\", \"y\"], skipna=True).compute()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'date': vv_data.time.values,\n",
    "            'vv_mean': vv_data.values\n",
    "        })\n",
    "        df = df.sort_values('date')\n",
    "        \n",
    "        # Remove NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        print(f\"  Successfully processed {len(df)} scenes\")\n",
    "        if len(df) > 0:\n",
    "            print(f\"  Mean VV backscatter: {df['vv_mean'].mean():.2f}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading with odc.stac: {str(e)[:200]}\")\n",
    "        return None\n",
    "\n",
    "# Process both regions\n",
    "print(\"\\nProcessing Region A (Vercelli, Italy)...\")\n",
    "df_region_a = get_s1_vv_mean_odc(REGION_A_BBOX, S1_DATE_RANGE, \"Region A\")\n",
    "\n",
    "print(\"\\nProcessing Region B (Pavia, Italy)...\")\n",
    "df_region_b = get_s1_vv_mean_odc(REGION_B_BBOX, S1_DATE_RANGE, \"Region B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c752cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the VV backscatter time series for both regions\n",
    "if df_region_a is not None and df_region_b is not None:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot Region A\n",
    "    ax.plot(df_region_a['date'], df_region_a['vv_mean'], \n",
    "            marker='o', linewidth=2.5, markersize=8,\n",
    "            label='Region A (Vercelli, Italy)', color='#3498db')\n",
    "    \n",
    "    # Plot Region B\n",
    "    ax.plot(df_region_b['date'], df_region_b['vv_mean'],\n",
    "            marker='s', linewidth=2.5, markersize=8,\n",
    "            label='Region B (Pavia, Italy)', color='#e67e22')\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('VV Backscatter (dB)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Sentinel-1 VV Backscatter: Region A vs Region B', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VV Backscatter Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nRegion A (Vercelli, Italy):\")\n",
    "    print(f\"  Mean VV: {df_region_a['vv_mean'].mean():.2f} dB\")\n",
    "    print(f\"  Std Dev: {df_region_a['vv_mean'].std():.2f} dB\")\n",
    "    print(f\"  Min VV: {df_region_a['vv_mean'].min():.2f} dB\")\n",
    "    print(f\"  Max VV: {df_region_a['vv_mean'].max():.2f} dB\")\n",
    "    \n",
    "    print(f\"\\nRegion B (Pavia, Italy):\")\n",
    "    print(f\"  Mean VV: {df_region_b['vv_mean'].mean():.2f} dB\")\n",
    "    print(f\"  Std Dev: {df_region_b['vv_mean'].std():.2f} dB\")\n",
    "    print(f\"  Min VV: {df_region_b['vv_mean'].min():.2f} dB\")\n",
    "    print(f\"  Max VV: {df_region_b['vv_mean'].max():.2f} dB\")\n",
    "    \n",
    "    print(f\"\\nDifference (Region A - Region B):\")\n",
    "    print(f\"  Mean difference: {(df_region_a['vv_mean'].mean() - df_region_b['vv_mean'].mean()):.2f} dB\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Could not process VV data for one or both regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491b80f",
   "metadata": {},
   "source": [
    "### Step 8: Comprehensive Availability Coverage\n",
    "Compute absolute data coverage counts across sensors. This produces graphical analytics outlining the feasibility of a multi-sensor yield prediction model.\n",
    "\n",
    "**Expected Output:** Deep heatmap visualizers, comparative mission bar-log charts, and terminal output describing raw STAC item saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up the STAC catalogs\n",
    "EOPF_STAC_URL = \"https://stac.core.eopf.eodc.eu\"\n",
    "AWS_STAC_URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "\n",
    "# Collections to query\n",
    "COLLECTIONS = {\n",
    "    \"Sentinel-1 GRD\": (\"sentinel-1-l1-grd\", EOPF_STAC_URL),\n",
    "    \"Sentinel-2 L2A\": (\"sentinel-2-l2a\", AWS_STAC_URL),\n",
    "    \"Sentinel-3 LST\": (\"sentinel-3-slstr-l2-lst\", EOPF_STAC_URL)\n",
    "}\n",
    "\n",
    "# Regions\n",
    "REGIONS = {\n",
    "    \"Region A (Vercelli, Italy)\": [8.34, 45.27, 8.40, 45.31],\n",
    "    \"Region B (Pavia, Italy)\": [9.022, 45.161, 9.055, 45.182]\n",
    "}\n",
    "\n",
    "# Date range for comprehensive analysis\n",
    "DATE_RANGE = \"2024-01-01/2025-12-31\"\n",
    "\n",
    "print(\"Fetching data availability for all Sentinel missions...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a519a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_counts(collection_name, collection_id, stac_url, bbox, date_range):\n",
    "    \"\"\"\n",
    "    Get monthly scene counts for a specific collection and region\n",
    "    \"\"\"\n",
    "    try:\n",
    "        catalog = pystac_client.Client.open(stac_url)\n",
    "        \n",
    "        # Add cloud cover filter for Sentinel-2\n",
    "        if \"sentinel-2\" in collection_id.lower():\n",
    "            search = catalog.search(\n",
    "                collections=[collection_id],\n",
    "                bbox=bbox,\n",
    "                datetime=date_range,\n",
    "                query={\"eo:cloud_cover\": {\"lt\": 50}}\n",
    "            )\n",
    "        else:\n",
    "            search = catalog.search(\n",
    "                collections=[collection_id],\n",
    "                bbox=bbox,\n",
    "                datetime=date_range\n",
    "            )\n",
    "        \n",
    "        items = list(search.items())\n",
    "        \n",
    "        if not items:\n",
    "            return pd.Series(dtype=int)\n",
    "        \n",
    "        # Extract dates\n",
    "        dates = [item.datetime for item in items]\n",
    "        df = pd.DataFrame({\"date\": pd.to_datetime(dates, utc=True)})\n",
    "        \n",
    "        # Group by year-month\n",
    "        monthly = df.groupby(df[\"date\"].dt.to_period(\"M\")).size().sort_index()\n",
    "        \n",
    "        return monthly\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {collection_name}: {str(e)[:100]}\")\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "# Collect data for all combinations\n",
    "all_data = {}\n",
    "\n",
    "for region_name, bbox in REGIONS.items():\n",
    "    print(f\"\\n{region_name}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    region_data = {}\n",
    "    \n",
    "    for collection_name, (collection_id, stac_url) in COLLECTIONS.items():\n",
    "        print(f\"  Fetching {collection_name}...\", end=\" \")\n",
    "        \n",
    "        monthly = get_monthly_counts(collection_name, collection_id, stac_url, bbox, DATE_RANGE)\n",
    "        \n",
    "        if len(monthly) > 0:\n",
    "            print(f\"\u2713 {len(monthly)} months with data, {monthly.sum()} total scenes\")\n",
    "            region_data[collection_name] = monthly\n",
    "        else:\n",
    "            print(\"\u2717 No data found\")\n",
    "            region_data[collection_name] = pd.Series(dtype=int)\n",
    "    \n",
    "    all_data[region_name] = region_data\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Data collection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive heatmap visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "for idx, (region_name, region_data) in enumerate(all_data.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    # Get all unique months across all collections\n",
    "    all_months = set()\n",
    "    for collection_name, monthly in region_data.items():\n",
    "        if len(monthly) > 0:\n",
    "            all_months.update(monthly.index)\n",
    "    \n",
    "    if not all_months:\n",
    "        ax.text(0.5, 0.5, f'No data available for {region_name}', \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax.set_title(region_name, fontsize=14, fontweight='bold', pad=15)\n",
    "        continue\n",
    "    \n",
    "    # Sort months\n",
    "    all_months = sorted(list(all_months))\n",
    "    \n",
    "    # Create matrix for heatmap\n",
    "    heatmap_data = []\n",
    "    collection_names = []\n",
    "    \n",
    "    for collection_name in [\"Sentinel-1 GRD\", \"Sentinel-2 L2A\", \"Sentinel-3 LST\"]:\n",
    "        if collection_name in region_data:\n",
    "            monthly = region_data[collection_name]\n",
    "            row = []\n",
    "            for month in all_months:\n",
    "                if month in monthly.index:\n",
    "                    row.append(monthly[month])\n",
    "                else:\n",
    "                    row.append(0)\n",
    "            heatmap_data.append(row)\n",
    "            collection_names.append(collection_name)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    heatmap_array = np.array(heatmap_data)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_array, \n",
    "                ax=ax,\n",
    "                cmap='YlOrRd',\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar_kws={'label': 'Number of Scenes'},\n",
    "                linewidths=0.5,\n",
    "                linecolor='gray')\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_yticklabels(collection_names, rotation=0, fontsize=11)\n",
    "    ax.set_xticklabels([str(m) for m in all_months], rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_xlabel('Month', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Sentinel Mission', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{region_name} - Data Availability Heatmap', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfce5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed statistics\n",
    "for region_name, region_data in all_data.items():\n",
    "    print(f\"\\n{region_name}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for collection_name in [\"Sentinel-1 GRD\", \"Sentinel-2 L2A\", \"Sentinel-3 LST\"]:\n",
    "        if collection_name in region_data:\n",
    "            monthly = region_data[collection_name]\n",
    "            \n",
    "            if len(monthly) > 0:\n",
    "                total_scenes = monthly.sum()\n",
    "                avg_per_month = monthly.mean()\n",
    "                max_month = monthly.idxmax()\n",
    "                max_scenes = monthly.max()\n",
    "                min_scenes = monthly.min()\n",
    "                \n",
    "                print(f\"\\n  {collection_name}:\")\n",
    "                print(f\"    Total scenes: {total_scenes}\")\n",
    "                print(f\"    Average per month: {avg_per_month:.1f}\")\n",
    "                print(f\"    Max scenes in a month: {max_scenes} ({max_month})\")\n",
    "                print(f\"    Min scenes in a month: {min_scenes}\")\n",
    "                print(f\"    Months with data: {len(monthly)}\")\n",
    "            else:\n",
    "                print(f\"\\n  {collection_name}:\")\n",
    "                print(f\"    No data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison bar chart showing total scenes by mission and region\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "regions = list(all_data.keys())\n",
    "missions = [\"Sentinel-1 GRD\", \"Sentinel-2 L2A\", \"Sentinel-3 LST\"]\n",
    "\n",
    "# Prepare data\n",
    "data_for_plot = {mission: [] for mission in missions}\n",
    "\n",
    "for region_name in regions:\n",
    "    region_data = all_data[region_name]\n",
    "    for mission in missions:\n",
    "        if mission in region_data and len(region_data[mission]) > 0:\n",
    "            data_for_plot[mission].append(region_data[mission].sum())\n",
    "        else:\n",
    "            data_for_plot[mission].append(0)\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(regions))\n",
    "width = 0.25\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e67e22']\n",
    "\n",
    "for i, (mission, color) in enumerate(zip(missions, colors)):\n",
    "    offset = width * (i - 1)\n",
    "    ax.bar(x + offset, data_for_plot[mission], width, \n",
    "           label=mission, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Region', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Number of Scenes', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Total Data Availability by Sentinel Mission and Region', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(regions, fontsize=10)\n",
    "ax.legend(fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Heatmap analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}